{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\igor-\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_runtime.py:185: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from importlib.metadata import version\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "# from lib.layerwrapper import WrappedGPT\n",
    "from lib.data import get_loaders \n",
    "\n",
    "from lib.prune_opt import prune_magnitude, prune_sparsegpt, prune_ablate, check_sparsity, find_layers\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args(object):\n",
    "    model: str = \"facebook/opt-125m\"\n",
    "    seed: int = 0\n",
    "    nsamples: int = 128\n",
    "    sparsity_ratio = 0.3\n",
    "    sparsity_type = \"unstructured\"\n",
    "    cache_dir = \"llm_weights\"\n",
    "    prune_method = \"wanda\"\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedGPT:\n",
    "    \"\"\"\n",
    "    This class wraps a GPT layer for specific operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, layer_id=0, layer_name=\"none\"):\n",
    "        \n",
    "        self.layer = layer\n",
    "        \n",
    "        self.dev = self.layer.weight.device\n",
    "        self.rows = layer.weight.data.shape[0]\n",
    "        self.columns = layer.weight.data.shape[1]\n",
    "\n",
    "        self.scaler_row = torch.zeros((self.columns), device=self.dev)\n",
    "        print(\"Scaler row shape (1): \", self.scaler_row.shape)\n",
    "        print(\"Scaler row max (1): \", torch.max(self.scaler_row))\n",
    "        print(\" \")\n",
    "        self.nsamples = 0\n",
    "\n",
    "        self.layer_id = layer_id \n",
    "        self.layer_name = layer_name\n",
    "\n",
    "    def add_batch(self, inp, out):\n",
    "        # if self.layer_name == \"self_attn.q_proj\":\n",
    "        #     print(\"self.nsamples: \", self.nsamples)\n",
    "        #     print(\"### LAYER NAME ###: \", self.layer_name)\n",
    "        # print(\"inp shape: \", inp.shape)\n",
    "\n",
    "        if len(inp.shape) == 2:\n",
    "            inp = inp.unsqueeze(0)\n",
    "        \n",
    "        tmp = inp.shape[0]\n",
    "        \n",
    "        if tmp != 1:\n",
    "            print(\"#### TMP ####: \", tmp)\n",
    "\n",
    "        if isinstance(self.layer, nn.Linear):\n",
    "            if len(inp.shape) == 3:\n",
    "                inp = inp.reshape((-1, inp.shape[-1]))\n",
    "                # print(\"Reshaped inp: \", inp.shape)\n",
    "            inp = inp.t()\n",
    "\n",
    "        self.scaler_row *= self.nsamples / (self.nsamples+tmp)\n",
    "\n",
    "        # print(\"Scaler row shape (2): \", self.scaler_row.shape)\n",
    "        # if self.layer_name == \"self_attn.q_proj\":\n",
    "        #     print(\"Scaler row norm (2): \", torch.norm(self.scaler_row, p = 2))\n",
    "        \n",
    "        self.nsamples += tmp\n",
    "        \n",
    "\n",
    "        inp = inp.type(torch.float32)\n",
    "\n",
    "        self.scaler_row += torch.norm(inp, p=2, dim=1) ** 2  / self.nsamples\n",
    "\n",
    "        # print(\"Scaler row shape (3): \", self.scaler_row.shape)\n",
    "        # if self.layer_name == \"self_attn.q_proj\":\n",
    "        #     print(\"Scaler row norm (3): \", torch.norm(self.scaler_row, p = 2))\n",
    "        #     print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm(model_name, cache_dir=\"llm_weights\"):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        torch_dtype=torch.float16, \n",
    "        cache_dir=cache_dir, \n",
    "        low_cpu_mem_usage=True, \n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    model.seqlen = model.config.max_position_embeddings \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_calibration_input(model, dataloader, device):\n",
    "\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    layers = model.model.decoder.layers\n",
    "    \n",
    "    # print(\"model: \", model)\n",
    "    # print(\"layers type: \", type(layers))\n",
    "    # print(\"layers: \", layers)\n",
    "    # print(\"layers[0] type: \", type(layers[0]))\n",
    "    # print(\"layers[0]: \", layers[0])\n",
    "    \n",
    "    if \"model.embed_tokens\" in model.hf_device_map:\n",
    "        device = model.hf_device_map[\"model.embed_tokens\"]\n",
    "\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    inps = torch.zeros((128, model.seqlen, model.config.hidden_size), dtype=dtype, device=device)\n",
    "    inps.requires_grad = False\n",
    "    \n",
    "    cache = {'i': 0, 'attention_mask': None, \"position_ids\": None}\n",
    "\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "\n",
    "            # print(\"module type: \", type(module))\n",
    "            # print(\"module: \", module)\n",
    "\n",
    "        def forward(self, inp, **kwargs):\n",
    "\n",
    "            # print(\"inp type: \", type(inp))\n",
    "            # print(\"inp shape: \", inp.shape)\n",
    "            # print(\"cache['i']: \", cache['i'])\n",
    "            # print(\"kwargs keys: \", list(kwargs.keys()))\n",
    "            inps[cache['i']] = inp\n",
    "\n",
    "            cache['i'] += 1\n",
    "            cache['attention_mask'] = kwargs['attention_mask']\n",
    "\n",
    "            raise ValueError\n",
    "        \n",
    "    layers[0] = Catcher(layers[0])\n",
    "    count = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        count += 1\n",
    "        try:\n",
    "\n",
    "            # print(\"batch[1] dtype: \", batch[1].dtype)\n",
    "            # print(\"batch[1] shape: \", batch[1].shape)\n",
    "            # print(\"batch[1] max: \", torch.max(batch[1]))\n",
    "            \n",
    "            # print(\"batch[0] dtype: \", batch[0].dtype)\n",
    "            # print(\"batch[0] shape: \", batch[0].shape)\n",
    "            # print(\"batch[0] max: \", torch.max(batch[0]))\n",
    "\n",
    "            model(batch[0].to(device))\n",
    "\n",
    "        except ValueError:\n",
    "            pass\n",
    "        # print(count)    \n",
    "\n",
    "    layers[0] = layers[0].module\n",
    "\n",
    "    outs = torch.zeros_like(inps)\n",
    "    attention_mask = cache['attention_mask']\n",
    "    model.config.use_cache = use_cache\n",
    "\n",
    "    return inps, outs, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_wanda(args, model, tokenizer, device=torch.device(\"cuda:0\"), prune_n=0, prune_m=0):\n",
    "    \n",
    "    use_cache = model.config.use_cache \n",
    "    model.config.use_cache = False \n",
    "    \n",
    "    print(\"loading calibdation data\")\n",
    "\n",
    "    dataloader, _ = get_loaders(\"c4\",nsamples=args.nsamples,seed=args.seed,seqlen=model.seqlen,tokenizer=tokenizer)\n",
    "\n",
    "    print(\"dataset loading complete\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inps, outs, attention_mask = prepare_calibration_input(model, dataloader, device)\n",
    "    print(\"att mask: \", attention_mask)\n",
    "    print(\"att mask type: \", type(attention_mask))\n",
    "    print(\"att mask type: \", attention_mask.shape)\n",
    "    layers = model.model.decoder.layers\n",
    "    \n",
    "    for i in range(len(layers)):\n",
    "\n",
    "        layer = layers[i]\n",
    "        subset = find_layers(layer)\n",
    "\n",
    "        if f\"model.layers.{i}\" in model.hf_device_map:   ## handle the case for llama-30B and llama-65B, when the device map has multiple GPUs;\n",
    "            dev = model.hf_device_map[f\"model.layers.{i}\"]\n",
    "            inps, outs, attention_mask = inps.to(dev), outs.to(dev), attention_mask.to(dev)\n",
    "\n",
    "        wrapped_layers = {}\n",
    "\n",
    "        \n",
    "        for name in subset:\n",
    "            print(\"name :\", name)\n",
    "            wrapped_layers[name] = WrappedGPT(subset[name], layer_name = name)\n",
    "\n",
    "        def add_batch(name):\n",
    "            def tmp(_, inp, out):\n",
    "                wrapped_layers[name].add_batch(inp[0].data, out.data)\n",
    "            return tmp\n",
    "\n",
    "        handles = []\n",
    "        \n",
    "        # why do we need this second loop?\n",
    "        # it could be absorbed into the first loop\n",
    "        \n",
    "        for name in wrapped_layers:\n",
    "            handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
    "            \n",
    "        for j in range(args.nsamples):\n",
    "            with torch.no_grad():\n",
    "                outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
    "        for h in handles:\n",
    "            h.remove()\n",
    "\n",
    "        for name in subset:\n",
    "            print(f\"pruning layer {i} name {name}\")\n",
    "            W_metric = torch.abs(subset[name].weight.data) * torch.sqrt(wrapped_layers[name].scaler_row.reshape((1,-1)))\n",
    "\n",
    "            W_mask = (torch.zeros_like(W_metric) == 1)  ## initialize a mask to be all False\n",
    "            if prune_n != 0:\n",
    "                # structured n:m sparsity\n",
    "                for ii in range(W_metric.shape[1]):\n",
    "                    if ii % prune_m == 0:\n",
    "                        tmp = W_metric[:,ii:(ii+prune_m)].float()\n",
    "                        W_mask.scatter_(1,ii+torch.topk(tmp, prune_n,dim=1, largest=False)[1], True)\n",
    "            else:\n",
    "                sort_res = torch.sort(W_metric, dim=-1, stable=True)\n",
    "\n",
    "                # unstructured pruning\n",
    "                indices = sort_res[1][:,:int(W_metric.shape[1]*args.sparsity_ratio)]\n",
    "                W_mask.scatter_(1, indices, True)\n",
    "\n",
    "            subset[name].weight.data[W_mask] = 0  ## set weights to zero \n",
    "\n",
    "        for j in range(args.nsamples):\n",
    "            with torch.no_grad():\n",
    "                outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
    "        inps, outs = outs, inps\n",
    "\n",
    "    model.config.use_cache = use_cache \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading llm model facebook/opt-125m\n"
     ]
    }
   ],
   "source": [
    "# Setting seeds for reproducibility\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.random.manual_seed(args.seed)\n",
    "\n",
    "# Handling n:m sparsity\n",
    "\n",
    "prune_n, prune_m = 0, 0\n",
    "model_name = args.model.split(\"/\")[-1]\n",
    "\n",
    "print(f\"loading llm model {args.model}\")\n",
    "\n",
    "model = get_llm(args.model, args.cache_dir)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model, use_fast=False)\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ppl_wikitext(model, testenc, bs=1, device=None):\n",
    "    # Get input IDs\n",
    "    testenc = testenc.input_ids\n",
    "\n",
    "    # Calculate number of samples\n",
    "    nsamples = testenc.numel() // model.seqlen\n",
    "\n",
    "    # List to store negative log likelihoods\n",
    "    nlls = []\n",
    "    print(f\"nsamples {nsamples}\")\n",
    "\n",
    "    # Loop through each batch\n",
    "    for i in range(0,nsamples,bs):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"sample {i}\")\n",
    "\n",
    "        # Calculate end index\n",
    "        j = min(i+bs, nsamples)\n",
    "\n",
    "        # Prepare inputs and move to device\n",
    "        inputs = testenc[:,(i * model.seqlen):(j * model.seqlen)].to(device)\n",
    "        inputs = inputs.reshape(j-i, model.seqlen)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        lm_logits = model(inputs).logits\n",
    "\n",
    "        # Shift logits and labels for next token prediction\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        shift_labels = inputs[:, 1:]\n",
    "\n",
    "        # Compute loss\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
    "\n",
    "        # Calculate negative log likelihood\n",
    "        neg_log_likelihood = loss.float() * model.seqlen * (j-i)\n",
    "\n",
    "        # Append to list of negative log likelihoods\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    # Compute perplexity\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n",
    "\n",
    "    # Empty CUDA cache to save memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ppl(args, model, tokenizer, dataset = \"c4\", device=torch.device(\"cuda:0\")):\n",
    "\n",
    "    # Print status\n",
    "    print(f\"evaluating on {dataset}\")\n",
    "\n",
    "    # Get the test loader\n",
    "    _, testloader = get_loaders(\n",
    "        dataset, seed=0, seqlen=model.seqlen, tokenizer=tokenizer \n",
    "    )\n",
    "\n",
    "    # Evaluate ppl in no grad context to avoid updating the model\n",
    "    with torch.no_grad():\n",
    "        ppl_test = eval_ppl_wikitext(model, testloader, 1, device)\n",
    "    return ppl_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on c4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration allenai--c4-ec45c889631c3c39\n",
      "Reusing dataset json (C:\\Users\\igor-\\.cache\\huggingface\\datasets\\json\\allenai--c4-ec45c889631c3c39\\0.0.0\\c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n",
      "Using custom data configuration allenai--c4-7700d5d1c53cf32f\n",
      "Reusing dataset json (C:\\Users\\igor-\\.cache\\huggingface\\datasets\\json\\allenai--c4-7700d5d1c53cf32f\\0.0.0\\c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples 256\n",
      "sample 0\n",
      "sample 50\n",
      "sample 100\n",
      "sample 150\n",
      "sample 200\n",
      "sample 250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26.56388282775879"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ppl(args, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on c4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration allenai--c4-ec45c889631c3c39\n",
      "Reusing dataset json (C:\\Users\\igor-\\.cache\\huggingface\\datasets\\json\\allenai--c4-ec45c889631c3c39\\0.0.0\\c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n",
      "Using custom data configuration allenai--c4-7700d5d1c53cf32f\n",
      "Reusing dataset json (C:\\Users\\igor-\\.cache\\huggingface\\datasets\\json\\allenai--c4-7700d5d1c53cf32f\\0.0.0\\c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples 256\n",
      "sample 0\n",
      "sample 50\n",
      "sample 100\n",
      "sample 150\n",
      "sample 200\n",
      "sample 250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26.56388282775879"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ppl(args, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use device  cuda:0\n",
      "pruning starts\n",
      "loading calibdation data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration allenai--c4-ec45c889631c3c39\n",
      "Reusing dataset json (C:\\Users\\igor-\\.cache\\huggingface\\datasets\\json\\allenai--c4-ec45c889631c3c39\\0.0.0\\c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n",
      "Using custom data configuration allenai--c4-7700d5d1c53cf32f\n",
      "Reusing dataset json (C:\\Users\\igor-\\.cache\\huggingface\\datasets\\json\\allenai--c4-7700d5d1c53cf32f\\0.0.0\\c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loading complete\n",
      "att mask:  tensor([[[[     0., -65504., -65504.,  ..., -65504., -65504., -65504.],\n",
      "          [     0.,      0., -65504.,  ..., -65504., -65504., -65504.],\n",
      "          [     0.,      0.,      0.,  ..., -65504., -65504., -65504.],\n",
      "          ...,\n",
      "          [     0.,      0.,      0.,  ...,      0., -65504., -65504.],\n",
      "          [     0.,      0.,      0.,  ...,      0.,      0., -65504.],\n",
      "          [     0.,      0.,      0.,  ...,      0.,      0.,      0.]]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "att mask type:  <class 'torch.Tensor'>\n",
      "att mask type:  torch.Size([1, 1, 2048, 2048])\n",
      "name : self_attn.k_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.v_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.q_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.out_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc1\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc2\n",
      "Scaler row shape (1):  torch.Size([3072])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "pruning layer 0 name self_attn.k_proj\n",
      "pruning layer 0 name self_attn.v_proj\n",
      "pruning layer 0 name self_attn.q_proj\n",
      "pruning layer 0 name self_attn.out_proj\n",
      "pruning layer 0 name fc1\n",
      "pruning layer 0 name fc2\n",
      "name : self_attn.k_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.v_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.q_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.out_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc1\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc2\n",
      "Scaler row shape (1):  torch.Size([3072])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "pruning layer 1 name self_attn.k_proj\n",
      "pruning layer 1 name self_attn.v_proj\n",
      "pruning layer 1 name self_attn.q_proj\n",
      "pruning layer 1 name self_attn.out_proj\n",
      "pruning layer 1 name fc1\n",
      "pruning layer 1 name fc2\n",
      "name : self_attn.k_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.v_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.q_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.out_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc1\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc2\n",
      "Scaler row shape (1):  torch.Size([3072])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "pruning layer 2 name self_attn.k_proj\n",
      "pruning layer 2 name self_attn.v_proj\n",
      "pruning layer 2 name self_attn.q_proj\n",
      "pruning layer 2 name self_attn.out_proj\n",
      "pruning layer 2 name fc1\n",
      "pruning layer 2 name fc2\n",
      "name : self_attn.k_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.v_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.q_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.out_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc1\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc2\n",
      "Scaler row shape (1):  torch.Size([3072])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "pruning layer 3 name self_attn.k_proj\n",
      "pruning layer 3 name self_attn.v_proj\n",
      "pruning layer 3 name self_attn.q_proj\n",
      "pruning layer 3 name self_attn.out_proj\n",
      "pruning layer 3 name fc1\n",
      "pruning layer 3 name fc2\n",
      "name : self_attn.k_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.v_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.q_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.out_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc1\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc2\n",
      "Scaler row shape (1):  torch.Size([3072])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "pruning layer 4 name self_attn.k_proj\n",
      "pruning layer 4 name self_attn.v_proj\n",
      "pruning layer 4 name self_attn.q_proj\n",
      "pruning layer 4 name self_attn.out_proj\n",
      "pruning layer 4 name fc1\n",
      "pruning layer 4 name fc2\n",
      "name : self_attn.k_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.v_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.q_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.out_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc1\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc2\n",
      "Scaler row shape (1):  torch.Size([3072])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "pruning layer 5 name self_attn.k_proj\n",
      "pruning layer 5 name self_attn.v_proj\n",
      "pruning layer 5 name self_attn.q_proj\n",
      "pruning layer 5 name self_attn.out_proj\n",
      "pruning layer 5 name fc1\n",
      "pruning layer 5 name fc2\n",
      "name : self_attn.k_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.v_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.q_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.out_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc1\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc2\n",
      "Scaler row shape (1):  torch.Size([3072])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "pruning layer 6 name self_attn.k_proj\n",
      "pruning layer 6 name self_attn.v_proj\n",
      "pruning layer 6 name self_attn.q_proj\n",
      "pruning layer 6 name self_attn.out_proj\n",
      "pruning layer 6 name fc1\n",
      "pruning layer 6 name fc2\n",
      "name : self_attn.k_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.v_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.q_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.out_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc1\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc2\n",
      "Scaler row shape (1):  torch.Size([3072])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "pruning layer 7 name self_attn.k_proj\n",
      "pruning layer 7 name self_attn.v_proj\n",
      "pruning layer 7 name self_attn.q_proj\n",
      "pruning layer 7 name self_attn.out_proj\n",
      "pruning layer 7 name fc1\n",
      "pruning layer 7 name fc2\n",
      "name : self_attn.k_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.v_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.q_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.out_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc1\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc2\n",
      "Scaler row shape (1):  torch.Size([3072])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "pruning layer 8 name self_attn.k_proj\n",
      "pruning layer 8 name self_attn.v_proj\n",
      "pruning layer 8 name self_attn.q_proj\n",
      "pruning layer 8 name self_attn.out_proj\n",
      "pruning layer 8 name fc1\n",
      "pruning layer 8 name fc2\n",
      "name : self_attn.k_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.v_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.q_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.out_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc1\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc2\n",
      "Scaler row shape (1):  torch.Size([3072])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "pruning layer 9 name self_attn.k_proj\n",
      "pruning layer 9 name self_attn.v_proj\n",
      "pruning layer 9 name self_attn.q_proj\n",
      "pruning layer 9 name self_attn.out_proj\n",
      "pruning layer 9 name fc1\n",
      "pruning layer 9 name fc2\n",
      "name : self_attn.k_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.v_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.q_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.out_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc1\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc2\n",
      "Scaler row shape (1):  torch.Size([3072])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "pruning layer 10 name self_attn.k_proj\n",
      "pruning layer 10 name self_attn.v_proj\n",
      "pruning layer 10 name self_attn.q_proj\n",
      "pruning layer 10 name self_attn.out_proj\n",
      "pruning layer 10 name fc1\n",
      "pruning layer 10 name fc2\n",
      "name : self_attn.k_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.v_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.q_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : self_attn.out_proj\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc1\n",
      "Scaler row shape (1):  torch.Size([768])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "name : fc2\n",
      "Scaler row shape (1):  torch.Size([3072])\n",
      "Scaler row max (1):  tensor(0., device='cuda:0')\n",
      " \n",
      "pruning layer 11 name self_attn.k_proj\n",
      "pruning layer 11 name self_attn.v_proj\n",
      "pruning layer 11 name self_attn.q_proj\n",
      "pruning layer 11 name self_attn.out_proj\n",
      "pruning layer 11 name fc1\n",
      "pruning layer 11 name fc2\n",
      "******************************\n",
      "layer 0 sparsity 0.299588\n",
      "layer 1 sparsity 0.299588\n",
      "layer 2 sparsity 0.299588\n",
      "layer 3 sparsity 0.299588\n",
      "layer 4 sparsity 0.299588\n",
      "layer 5 sparsity 0.299588\n",
      "layer 6 sparsity 0.299588\n",
      "layer 7 sparsity 0.299588\n",
      "layer 8 sparsity 0.299588\n",
      "layer 9 sparsity 0.299588\n",
      "layer 10 sparsity 0.299588\n",
      "layer 11 sparsity 0.299588\n",
      "sparsity sanity check 0.2996\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"use device \", device)\n",
    "\n",
    "if args.sparsity_ratio != 0:\n",
    "    print(\"pruning starts\")\n",
    "    if args.prune_method == \"wanda\":\n",
    "        prune_wanda(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m)\n",
    "    elif args.prune_method == \"magnitude\":\n",
    "        prune_magnitude(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m)\n",
    "    elif args.prune_method == \"sparsegpt\":\n",
    "        prune_sparsegpt(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m)\n",
    "    elif \"ablate\" in args.prune_method:\n",
    "        prune_ablate(args, model, tokenizer, device, prune_n=prune_n, prune_m=prune_m)\n",
    "\n",
    "################################################################\n",
    "\n",
    "print(\"*\"*30)\n",
    "sparsity_ratio = check_sparsity(model)\n",
    "print(f\"sparsity sanity check {sparsity_ratio:.4f}\")\n",
    "print(\"*\"*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on c4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration allenai--c4-ec45c889631c3c39\n",
      "Reusing dataset json (C:\\Users\\igor-\\.cache\\huggingface\\datasets\\json\\allenai--c4-ec45c889631c3c39\\0.0.0\\c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n",
      "Using custom data configuration allenai--c4-7700d5d1c53cf32f\n",
      "Reusing dataset json (C:\\Users\\igor-\\.cache\\huggingface\\datasets\\json\\allenai--c4-7700d5d1c53cf32f\\0.0.0\\c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nsamples 256\n",
      "sample 0\n",
      "sample 50\n",
      "sample 100\n",
      "sample 150\n",
      "sample 200\n",
      "sample 250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27.570693969726562"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ppl(args, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4905.3291015625"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "26.56388282775879 # baseline unpruned\n",
    "26.56388282775879 # loop absorption, unpruned\n",
    "26.56388282775879 # scaler_row commented, unpruned\n",
    "\n",
    "\n",
    "27.570693969726562 # baseline pruned\n",
    "27.570693969726562 # loop absorption, pruned\n",
    "4905.3291015625 # scaler_row commented, pruned\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
